# -*- coding: utf-8 -*-
"""CNN_newimage.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xAITneKZEYHdzNFK5PRqkrXTS0F7E0wA

# CNN

## Import MNIST Images - Deep Learning with PyTorch 14
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torchvision.utils import make_grid

import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

# %matplotlib inline

# Convert MNIST Image Files into a Tensor of 4-Dimensions (# of images, Height, Width, Color Channel)
transform = transforms.ToTensor()

# Train Data
train_data = datasets.MNIST(root='/cnn_data', train=True, download=True, transform=transform)

# test Data
test_data = datasets.MNIST(root='/cnn_data', train=False, download=True, transform=transform)

train_data

test_data

pwd

ls

cd ../

ls -al

cd cnn_data

ls -l

cd /

ls -l

cd content/

ls -al

"""## Convolutional and Pooling Layers - Deep Learning with PyTorch 15

"""

# Create a small batch size for images...  let's say 10
train_loader = DataLoader(train_data, batch_size=10, shuffle=True)
test_loader = DataLoader(test_data, batch_size=10, shuffle=False)

# Define the CNN Model
# Decribe the convolutional layer and what it's doing (2 convolutional layers)
# This is an example
conv1 = nn.Conv2d(1, 6, 3, 1)
conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3, stride=1)

# Grab 1 MNIST record/image
for i, (X_train, y_train) in enumerate(train_data):
  break

X_train

X_train.shape

x = X_train.view(1,1, 28, 28)

# Perform the first convolution
x = F.relu(conv1(x)) # Rectified Linear Unit for the activation function

x

# 1 is the single image, 6 is the filters asked for, 26x26
x.shape

# Pass through the pooling layer
x = F.max_pool2d(x, 2, 2) # Kernel of 2 and stride of 2

x.shape # 26 / 2 = 13

# Do the second convolutional layer
x = F.relu(conv2(x))

x.shape # Again, no padding was specified so 2 pixels were lost around the outside of the image

# Pooling layer
x = F.max_pool2d(x, 2, 2)

x.shape # 11 / 2 = 5.5 but it is rounded down because no data can invented to round up

(((28-2) / 2) -2) / 2

"""## Convolutional Neural Network Model - Deep Learning with PyTorch 16

"""

# Model Class
class ConvolutionalNetwork(nn.Module):
  def __init__(self) -> None:
    super().__init__()
    self.conv1 = nn.Conv2d(1, 6, 3, 1)
    self.conv2 = nn.Conv2d(6, 16, 3, 1)

    # Fully Connected Layers
    self.fc1 = nn.Linear(5*5*16, 120)
    self.fc2 = nn.Linear(120, 84)
    self.fc3 = nn.Linear(84, 10)

  def forward(self, X):
    X = F.relu(self.conv1(X))
    X = F.max_pool2d(X, 2, 2) # 2x2 kernel and stride = 2
    # Second pass
    X = F.relu(self.conv2(X))
    X = F.max_pool2d(X, 2, 2) # 2x2 kernel and stride = 2

    # Re-View the data to flatten it out
    X = X.view(-1, 16*5*5) # Negative one so the batch size can be varied

    # Fully Connected Layers
    X = F.relu(self.fc1(X))
    X = F.relu(self.fc2(X))
    X = self.fc3(X)

    return F.log_softmax(X, dim=1)

# Create an Instance of the Model
torch.manual_seed(41)
model = ConvolutionalNetwork()
model

# Loss Function Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # The smaller the learning rate, the longer it's going to take to train

"""## Train and Test CNN Model - Deep Learning with PyTorch 17"""

import time
start_time = time.time()

# Create Variables to track things
epochs = 5
train_losses = []
test_losses = []
train_correct = []
test_correct = []


# For Loop offor Epochs
for i in range(epochs):
  training_correct = 0
  testing_correct = 0

  # Train
  for b, (X_train, y_train) in enumerate(train_loader):
    b += 1 # Start the batches at 1

    y_pred = model(X_train) # Get the predicted values from the training set (data is 2d, not flattened.)
    loss = criterion(y_pred, y_train) # How off are we? Compare the predictions to the correct answers in y_train

    predicted = torch.max(y_pred.data, 1)[1] # Add up the number of correct predictions. Indexed off the first point
    batch_correct = (predicted == y_train).sum() # How many we got correct from this specific batch. True=1, False=0, sum those up.
    training_correct += batch_correct # Keep track as we go along in training.

    # Update the parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Print out some results
    if b % 600 == 0 :
      print(f'Epoch: {i} Batch: {b} Loss: {loss.item()}')


  train_losses.append(loss)
  train_correct.append(training_correct)

  # Test
  with torch.no_grad(): # No gradient so the weights and the bias are not updated with test data
    for b , (X_test, y_test) in enumerate(test_loader):
      y_val = model(X_test)
      predicted = torch.max(y_val.data, 1)[1] # Adding up correct predictions
      testing_correct += (predicted == y_test).sum() # True=1, False=0, sum all

  loss = criterion(y_val, y_test)
  test_losses.append(loss)
  test_correct.append(testing_correct)

current_time = time.time()
total = current_time - start_time
print(f'Training time: {total/60} minutes!')
total

# Graph the loss of Epoch
train_losses = [tl.item() for tl in train_losses]
plt.plot(train_losses, label='Training Loss')
plt.plot(test_losses, label='Validation Loss')
plt.title('Loss at Epoch')
plt.legend()

import time
start_time = time.time()

# Create Variables To Tracks Things
epochs = 5
train_losses = []
test_losses = []
train_correct = []
test_correct = []

# For Loop of Epochs
for i in range(epochs):
  trn_corr = 0
  tst_corr = 0


  # Train
  for b,(X_train, y_train) in enumerate(train_loader):
    b+=1 # start our batches at 1
    y_pred = model(X_train) # get predicted values from the training set. Not flattened 2D
    loss = criterion(y_pred, y_train) # how off are we? Compare the predictions to correct answers in y_train

    predicted = torch.max(y_pred.data, 1)[1] # add up the number of correct predictions. Indexed off the first point
    batch_corr = (predicted == y_train).sum() # how many we got correct from this batch. True = 1, False=0, sum those up
    trn_corr += batch_corr # keep track as we go along in training.

    # Update our parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()


    # Print out some results
    if b%600 == 0:
      print(f'Epoch: {i}  Batch: {b}  Loss: {loss.item()}')

  train_losses.append(loss)
  train_correct.append(trn_corr)


  # Test
  with torch.no_grad(): #No gradient so we don't update our weights and biases with test data
    for b,(X_test, y_test) in enumerate(test_loader):
      y_val = model(X_test)
      predicted = torch.max(y_val.data, 1)[1] # Adding up correct predictions
      tst_corr += (predicted == y_test).sum() # T=1 F=0 and sum away


  loss = criterion(y_val, y_test)
  test_losses.append(loss)
  test_correct.append(tst_corr)



current_time = time.time()
total = current_time - start_time
print(f'Training Took: {total/60} minutes!')

"""## Graph CNN Results - Deep Learning with PyTorch 18"""

# Graph the loss of Epoch
train_losses = [tl.item() for tl in train_losses]
plt.plot(train_losses, label='Training Loss')
plt.plot(test_losses, label='Validation Loss')
plt.title('Loss at Epoch')
plt.legend()

# Graph the accuracy at the end of each Epoch
plt.plot([t/600 for t in train_correct], label='Training Accuracy')
plt.plot([t/100 for t in test_correct], label='Validation Accuracy')
plt.title('Accuracy at the end of each Epoch')
plt.legend()

test_load_everything = DataLoader(test_data, batch_size=10_000, shuffle=False)

with torch.no_grad():
  correct = 0

  for X_test, y_test in test_load_everything:
    y_val = model(X_test)
    predicted = torch.max(y_val.data, 1)[1]
    correct += (predicted == y_test).sum()

# Did for correct (out of 10000)
correct.item()

# Did for correct (percentage of correct)
 correct.item() / len(test_data) * 100

"""## NSend New Image Thru The Model - Deep Learning with PyTorch 19

"""

# Grab an Image
test_data[4143] # Tensor with an image in it... at the end, it shows the label

# Grab just the data
test_data[4143][0]

# Reshape it
test_data[4143][0].reshape(28, 28)

# Show the image
plt.imshow(test_data[4143][0].reshape(28, 28))

# Pass the image through the model
model.eval()

with torch.no_grad():
  new_prediction = model(test_data[4143][0].view(1, 1, 28, 28)) # batch size=1, color channel=1, image size: 28x28

# Check the new prediction - get the probabilities
new_prediction

# Highest probability
new_prediction.argmax()

# Grab an Image
test_data[1978] # Tensor with an image in it... at the end, it shows the label

# Grab just the data
test_data[1978][0]

# Reshape it
test_data[1978][0].reshape(28, 28)

# Show the image
plt.imshow(test_data[1978][0].reshape(28, 28))

# Pass the image through the model
model.eval()

with torch.no_grad():
  new_prediction = model(test_data[1978][0].view(1, 1, 28, 28)) # batch size=1, color channel=1, image size: 28x28

# Check the new prediction - get the probabilities
new_prediction

# Highest probability
new_prediction.argmax()